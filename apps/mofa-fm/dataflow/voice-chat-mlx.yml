# Voice Chat Dataflow with MLX ASR (Paraformer)
#
# This dataflow uses the dora-asr-mlx node which provides:
# - Fast Chinese ASR (50-75x real-time)
# - MLX Metal acceleration (Apple Silicon only)
# - Same Paraformer model as Python funasr
#
# Use this dataflow for:
# - Chinese-only voice chat
# - Low latency requirements
# - Apple Silicon Macs

nodes:
  # ============ Study Participants (MaaS) ============

  - id: student1
    build: cargo build --release --manifest-path ../../../node-hub/dora-maas-client/Cargo.toml
    path: ../../../node-hub/dora-maas-client/target/release/dora-maas-client
    inputs:
      text: bridge-to-student1/text
      control: conference-controller/llm_control
    outputs:
      - text
      - status
      - log
    env:
      MAAS_CONFIG_PATH: study_config_student1.toml
      ALIBABA_CLOUD_API_KEY: ${ALIBABA_CLOUD_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY:-}
      NVIDIA_API_KEY: ${NVIDIA_API_KEY:-}
      LOG_LEVEL: INFO

  - id: student2
    build: cargo build --release --manifest-path ../../../node-hub/dora-maas-client/Cargo.toml
    path: ../../../node-hub/dora-maas-client/target/release/dora-maas-client
    inputs:
      text: bridge-to-student2/text
      control: conference-controller/llm_control
    outputs:
      - text
      - status
      - log
    env:
      MAAS_CONFIG_PATH: study_config_student2.toml
      ALIBABA_CLOUD_API_KEY: ${ALIBABA_CLOUD_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY:-}
      NVIDIA_API_KEY: ${NVIDIA_API_KEY:-}
      LOG_LEVEL: INFO

  - id: tutor
    build: cargo build --release --manifest-path ../../../node-hub/dora-maas-client/Cargo.toml
    path: ../../../node-hub/dora-maas-client/target/release/dora-maas-client
    inputs:
      text: bridge-to-tutor/text
      control: conference-controller/judge_prompt
    outputs:
      - text
      - status
      - log
    env:
      MAAS_CONFIG_PATH: study_config_tutor.toml
      ALIBABA_CLOUD_API_KEY: ${ALIBABA_CLOUD_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY:-}
      NVIDIA_API_KEY: ${NVIDIA_API_KEY:-}
      LOG_LEVEL: INFO

  # ============ Multi-Participant Audio Pipeline ============

  - id: multi-text-segmenter
    build: pip install -e ../../../node-hub/dora-text-segmenter
    path: dora-text-segmenter
    inputs:
      student1:
        source: student1/text
        queue_size: 1000
      student2:
        source: student2/text
        queue_size: 1000
      tutor:
        source: tutor/text
        queue_size: 1000
      audio_complete:
        source: mofa-audio-player/audio_complete
        queue_size: 100
      audio_buffer_control:
        source: mofa-audio-player/buffer_status
        queue_size: 10
      control: conference-controller/llm_control
      reset: conference-controller/control_judge
      immediate_cancel: conference-controller/segmenter_control
    outputs:
      - text_segment_student1
      - text_segment_student2
      - text_segment_tutor
      - status
      - metrics
      - log
    env:
      SEGMENTER_MODE: "conference"
      SEGMENT_MODE: "sentence"
      MIN_SEGMENT_LENGTH: "5"
      MAX_SEGMENT_LENGTH: "15"
      PUNCTUATION_MARKS: '。！？.!?，,、；：""''（）【】《》'
      REMOVE_SPEAKER_ID: "true"
      LOG_LEVEL: "DEBUG"
      AUDIO_BUFFER_LOW_WATER_MARK: "30"
      AUDIO_BUFFER_HIGH_WATER_MARK: "60"

  # PrimeSpeech TTS nodes (same as voice-chat.yml)
  - id: primespeech-student1
    build: pip install -e ../../../libs/dora-common -e ../../../node-hub/dora-primespeech
    path: dora-primespeech
    inputs:
      text: multi-text-segmenter/text_segment_student1
    outputs:
      - audio
      - status
      - segment_complete
      - log
    env:
      TRANSFORMERS_OFFLINE: "1"
      HF_HUB_OFFLINE: "1"
      VOICE_NAME: "Zhao Daniu"
      PRIMESPEECH_MODEL_DIR: $HOME/.dora/models/primespeech
      TEXT_LANG: zh
      PROMPT_LANG: zh
      TOP_K: 5
      TOP_P: 1.0
      TEMPERATURE: 1.0
      SPEED_FACTOR: 1.1
      FRAGMENT_INTERVAL: "0.2"
      USE_GPU: false
      NUM_THREADS: 4
      RETURN_FRAGMENT: "false"
      LOG_LEVEL: DEBUG
      ENABLE_INTERNAL_SEGMENTATION: "true"
      TTS_MAX_SEGMENT_LENGTH: "100"
      TTS_MIN_SEGMENT_LENGTH: "20"

  - id: primespeech-student2
    build: pip install -e ../../../libs/dora-common -e ../../../node-hub/dora-primespeech
    path: dora-primespeech
    inputs:
      text: multi-text-segmenter/text_segment_student2
    outputs:
      - audio
      - status
      - segment_complete
      - log
    env:
      TRANSFORMERS_OFFLINE: "1"
      HF_HUB_OFFLINE: "1"
      VOICE_NAME: "Doubao"
      PRIMESPEECH_MODEL_DIR: $HOME/.dora/models/primespeech
      TEXT_LANG: zh
      PROMPT_LANG: zh
      TOP_K: 5
      TOP_P: 1.0
      TEMPERATURE: 1.0
      SPEED_FACTOR: 1.1
      FRAGMENT_INTERVAL: "0.2"
      USE_GPU: false
      NUM_THREADS: 4
      RETURN_FRAGMENT: "false"
      LOG_LEVEL: DEBUG
      ENABLE_INTERNAL_SEGMENTATION: "true"
      TTS_MAX_SEGMENT_LENGTH: "100"
      TTS_MIN_SEGMENT_LENGTH: "20"

  - id: primespeech-tutor
    build: pip install -e ../../../libs/dora-common -e ../../../node-hub/dora-primespeech
    path: dora-primespeech
    inputs:
      text: multi-text-segmenter/text_segment_tutor
    outputs:
      - audio
      - status
      - segment_complete
      - log
    env:
      TRANSFORMERS_OFFLINE: "1"
      HF_HUB_OFFLINE: "1"
      VOICE_NAME: "Ma Yun"
      PRIMESPEECH_MODEL_DIR: $HOME/.dora/models/primespeech
      TEXT_LANG: zh
      PROMPT_LANG: zh
      TOP_K: 5
      TOP_P: 1.0
      TEMPERATURE: 1.0
      SPEED_FACTOR: 1.1
      FRAGMENT_INTERVAL: "0.1"
      USE_GPU: false
      NUM_THREADS: 4
      RETURN_FRAGMENT: "false"
      LOG_LEVEL: DEBUG
      ENABLE_INTERNAL_SEGMENTATION: "true"
      TTS_MAX_SEGMENT_LENGTH: "100"
      TTS_MIN_SEGMENT_LENGTH: "20"

  # ============ Conference Bridges ============

  - id: bridge-to-student1
    build: cargo build --release --manifest-path ../../../node-hub/dora-conference-bridge/Cargo.toml
    path: ../../../node-hub/dora-conference-bridge/target/release/dora-conference-bridge
    env:
      LOG_LEVEL: INFO
      STREAMING_PORTS: student1,tutor,student2,human
      ERROR_MESSAGE_TEMPLATE: "[{participant} is experiencing technical difficulties. We will proceed without their response.]"
      DORA_STUDY_MODE: "true"
    inputs:
      student2:
        source: student2/text
        queue_size: 1000
      tutor:
        source: tutor/text
        queue_size: 1000
      human:
        source: asr-mlx/transcription
        queue_size: 1000
      control:
        source: conference-controller/control_llm1
        queue_size: 10
    outputs:
      - text
      - status
      - log

  - id: bridge-to-student2
    build: cargo build --release --manifest-path ../../../node-hub/dora-conference-bridge/Cargo.toml
    path: ../../../node-hub/dora-conference-bridge/target/release/dora-conference-bridge
    env:
      LOG_LEVEL: INFO
      STREAMING_PORTS: student1,tutor,student2,human
      ERROR_MESSAGE_TEMPLATE: "[{participant} is experiencing technical difficulties. We will proceed without their response.]"
      DORA_STUDY_MODE: "true"
    inputs:
      student1:
        source: student1/text
        queue_size: 1000
      tutor:
        source: tutor/text
        queue_size: 1000
      human:
        source: asr-mlx/transcription
        queue_size: 1000
      control:
        source: conference-controller/control_llm2
        queue_size: 10
    outputs:
      - text
      - status
      - log

  - id: bridge-to-tutor
    build: cargo build --release --manifest-path ../../../node-hub/dora-conference-bridge/Cargo.toml
    path: ../../../node-hub/dora-conference-bridge/target/release/dora-conference-bridge
    env:
      LOG_LEVEL: INFO
      STREAMING_PORTS: student1,tutor,student2,human
      ERROR_MESSAGE_TEMPLATE: "[{participant} is experiencing technical difficulties. We will proceed without their response.]"
      DORA_STUDY_MODE: "true"
    inputs:
      student1:
        source: student1/text
        queue_size: 1000
      student2:
        source: student2/text
        queue_size: 1000
      human:
        source: asr-mlx/transcription
        queue_size: 1000
      control:
        source: conference-controller/control_judge
        queue_size: 10
    outputs:
      - text
      - status
      - log

  # ============ Conference Controller ============

  - id: conference-controller
    build: cargo build --release --manifest-path ../../../node-hub/dora-conference-controller/Cargo.toml
    path: ../../../node-hub/dora-conference-controller/target/release/dora-conference-controller
    env:
      DORA_POLICY_PATTERN: "[(human, 0.001), (tutor, *), (student1, 1), (student2, 1)]"
      INITIAL_QUESTION_ID: 1
      LOG_LEVEL: "DEBUG"
      AUDIO_BUFFER_THRESHOLD: 30
      AUDIO_BUFFER_RESUME_THRESHOLD: 10
    inputs:
      student1:
        source: student1/text
        queue_size: 1000
      student2:
        source: student2/text
        queue_size: 1000
      tutor:
        source: tutor/text
        queue_size: 1000
      human:
        source: asr-mlx/transcription
        queue_size: 1000
      control: mofa-prompt-input/control
      session_start: mofa-audio-player/session_start
      human_speaking: mofa-mic-input/speech_started
      question_ended: mofa-mic-input/question_ended
      buffer_status: mofa-audio-player/buffer_status
    outputs:
      - control_judge
      - control_llm2
      - control_llm1
      - llm_control
      - judge_prompt
      - segmenter_control
      - status
      - log

  # ============ MoFA Dynamic Nodes ============

  - id: mofa-mic-input
    path: dynamic
    outputs:
      - audio_segment
      - speech_started
      - speech_ended
      - is_speaking
      - question_ended
      - status
      - log

  # ============ MLX ASR Node (Paraformer) ============
  - id: asr-mlx
    build: cargo build --release --manifest-path ../../../node-hub/dora-asr-mlx/Cargo.toml
    path: ../../../node-hub/dora-asr-mlx/target/release/dora-asr-mlx
    inputs:
      audio: mofa-mic-input/audio_segment
    outputs:
      - transcription
      - language_detected
      - processing_time
      - log
    env:
      PARAFORMER_MODEL_DIR: $HOME/.mofa/models/paraformer-large-mlx
      MIN_AUDIO_DURATION: "0.5"
      MAX_AUDIO_DURATION: "30.0"
      ASR_MLX_WARMUP: "true"
      LOG_LEVEL: "INFO"

  - id: mofa-audio-player
    path: dynamic
    inputs:
      audio_student1:
        source: primespeech-student1/audio
        queue_size: 1000
      audio_student2:
        source: primespeech-student2/audio
        queue_size: 1000
      audio_tutor:
        source: primespeech-tutor/audio
        queue_size: 1000
      reset:
        source: conference-controller/llm_control
        queue_size: 10
    outputs:
      - buffer_status
      - status
      - session_start
      - audio_complete
      - log

  - id: mofa-prompt-input
    path: dynamic
    env:
      DORA_STUDY_MODE: "true"
      LOG_LEVEL: "DEBUG"
    inputs:
      llm1_text:
        source: student1/text
        queue_size: 1000
      llm2_text:
        source: student2/text
        queue_size: 1000
      judge_text:
        source: tutor/text
        queue_size: 1000
      human_text:
        source: asr-mlx/transcription
        queue_size: 1000
      human_status: asr-mlx/log
    outputs:
      - control

  - id: mofa-system-log
    path: dynamic
    inputs:
      llm1_log:
        source: student1/log
        queue_size: 1000
      llm1_status: student1/status
      llm1_text:
        source: student1/text
        queue_size: 1000
      llm2_log:
        source: student2/log
        queue_size: 1000
      llm2_status: student2/status
      llm2_text:
        source: student2/text
        queue_size: 1000
      judge_log:
        source: tutor/log
        queue_size: 1000
      judge_status: tutor/status
      judge_text:
        source: tutor/text
        queue_size: 1000
      bridge1_log:
        source: bridge-to-student1/log
        queue_size: 1000
      bridge1_status: bridge-to-student1/status
      bridge1_text:
        source: bridge-to-student1/text
        queue_size: 1000
      bridge2_log:
        source: bridge-to-student2/log
        queue_size: 1000
      bridge2_status: bridge-to-student2/status
      bridge2_text:
        source: bridge-to-student2/text
        queue_size: 1000
      bridge3_log:
        source: bridge-to-tutor/log
        queue_size: 1000
      bridge3_status: bridge-to-tutor/status
      bridge3_text:
        source: bridge-to-tutor/text
        queue_size: 1000
      controller_status: conference-controller/status
      controller_log:
        source: conference-controller/log
        queue_size: 1000
      control_judge: conference-controller/control_judge
      control_llm2: conference-controller/control_llm2
      control_llm1: conference-controller/control_llm1
      segmenter_log:
        source: multi-text-segmenter/log
        queue_size: 1000
      segmenter_status: multi-text-segmenter/status
      tts1_log:
        source: primespeech-student1/log
        queue_size: 1000
      tts1_status: primespeech-student1/status
      tts2_log:
        source: primespeech-student2/log
        queue_size: 1000
      tts2_status: primespeech-student2/status
      tts3_log:
        source: primespeech-tutor/log
        queue_size: 1000
      tts3_status: primespeech-tutor/status
      audio_status: mofa-audio-player/status
      audio_player_log:
        source: mofa-audio-player/log
        queue_size: 1000
      mic_input_log:
        source: mofa-mic-input/log
        queue_size: 1000
      mic_input_status: mofa-mic-input/status
      asr_log:
        source: asr-mlx/log
        queue_size: 1000
      human_text:
        source: asr-mlx/transcription
        queue_size: 1000
